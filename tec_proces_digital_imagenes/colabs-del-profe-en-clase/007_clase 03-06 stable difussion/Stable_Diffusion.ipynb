{"cells":[{"cell_type":"markdown","id":"intro-titulo-clase","metadata":{"id":"intro-titulo-clase"},"source":["# Introducción a los Modelos de Difusión y Stable Diffusion"]},{"cell_type":"markdown","id":"teoria-que-son","metadata":{"id":"teoria-que-son"},"source":["## ¿Qué son los Modelos de Difusión?\n","\n","En su esencia, los Modelos de Difusión son una clase de **modelos generativos** que han revolucionado la creación de contenido visual. A diferencia de otros enfoques, no intentan generar una imagen desde cero directamente, sino que aprenden a **deshacer una imagen en un proceso de ruido gradual**.\n","\n","Imaginen que tienen una imagen original perfecta. Los modelos de difusión aprenden a añadirle ruido gradualmente hasta que se convierte en puro ruido aleatorio. Luego, invierten este proceso: aprenden a eliminar ese ruido paso a paso hasta recuperar la imagen original (o generar una nueva)."]},{"cell_type":"markdown","id":"teoria-para-que-se-usan","metadata":{"id":"teoria-para-que-se-usan"},"source":["## ¿Para qué se utilizan? Su aplicación principal es la **generación de imágenes realistas** a partir de texto (text-to-image) o a partir de otras imágenes. Sin embargo, su versatilidad va más allá, permitiendo tareas como:\n","*   **Generación de imágenes:** Crear imágenes nuevas y diversas a partir de una descripción textual (ej. Stable Diffusion, DALL-E 2, Midjourney).*   \n","\n","* **Edición de imágenes:** Modificar partes de una imagen, completar secciones faltantes (inpainting), o eliminar objetos no deseados (outpainting).*   \n","\n","* **Super-resolución:** Mejorar la calidad y resolución de imágenes de baja calidad.*   \n","\n","* **Transferencia de estilo:** Aplicar el estilo de una imagen a otra."]},{"cell_type":"markdown","id":"teoria-como-funcionan","metadata":{"id":"teoria-como-funcionan"},"source":["## ¿Cómo funcionan (de forma simplificada)?\n","###Los Modelos de Difusión operan en dos fases principales, inspiradas en procesos físicos de difusión (como la propagación de calor o el movimiento de partículas):\n","\n","1.  **Proceso de Difusión (o Adelante/Forward Process):**    *   Este proceso es determinista (conocido) y no se entrena.    *   Toma una imagen original (limpia) y le **añade ruido gaussiano de forma gradual** en una secuencia de pasos.    *   En cada paso, se añade una pequeña cantidad de ruido.    *   Después de muchos pasos, la imagen original se degrada completamente hasta convertirse en puro ruido aleatorio.\n","2.  **Proceso Inverso (o de Denoising/Reverse Process):**    *   Este es el proceso que el modelo **aprende** a realizar.    *   Toma una imagen ruidosa (que puede ser ruido puro al inicio, o una imagen con ruido añadido) y aprende a **eliminar ese ruido progresivamente** en cada paso.    *   El modelo es una red neuronal (a menudo una U-Net) que se entrena para predecir el ruido añadido en cada paso de reversión.    *   Al \"substraer\" el ruido predicho iterativamente, la red puede transformar una señal ruidosa en una imagen coherente y realista.    *   Cuando generamos una imagen nueva, empezamos con un vector de ruido aleatorio y el modelo lo \"limpia\" paso a paso hasta formar una imagen."]},{"cell_type":"markdown","id":"teoria-por-que-potentes","metadata":{"id":"teoria-por-que-potentes"},"source":["## ¿Por qué son tan potentes?*   \n","\n","**Calidad de imagen:** Son capaces de generar imágenes con un detalle y realismo excepcionales, a menudo superando a otros modelos generativos.*  \n","\n","**Diversidad:** Pueden generar una amplia variedad de imágenes dado un mismo *prompt* o condición, lo que indica una buena exploración del espacio de datos.*   \n","\n","**Estabilidad en el entrenamiento:** Tienden a ser más estables y fáciles de entrenar que otras arquitecturas generativas como las GANs (Generative Adversarial Networks), que a menudo sufren de modos colapsados.*   \n","\n","**Control granular:** Al operar paso a paso, ofrecen un mayor control sobre el proceso de generación y permiten intervenciones para guiar la salida."]},{"cell_type":"markdown","id":"seccion-stable-diffusion","metadata":{"id":"seccion-stable-diffusion"},"source":["## Explorando Stable Diffusion: Generación de Imágenes a partir de Texto\n","\n","En este cuaderno exploraremos el uso práctico de **Stable Diffusion** para la generación de imágenes a partir de descripciones de texto (`prompts`). Esta es una potente herramienta de IA que nos permite generar imágenes altamente detalladas y creativas.\n","\n","Comenzaremos con la preparación del entorno, luego exploraremos dos versiones del modelo (V1.5 y 2.1) y finalmente veremos cómo ajustar los parámetros para obtener resultados personalizados."]},{"cell_type":"markdown","id":"preparacion-entorno","metadata":{"id":"preparacion-entorno"},"source":["### 1. Preparación del Entorno\n","Antes de comenzar, necesitamos instalar las librerías necesarias para trabajar con los modelos de difusión y configurar nuestro entorno para aprovechar la GPU (si está disponible en Google Colab)."]},{"cell_type":"code","execution_count":null,"id":"paso-1-instalar-librerias","metadata":{"id":"paso-1-instalar-librerias"},"outputs":[],"source":["# Paso 1: Instalar las librerías necesarias\n","# 'diffusers' es la biblioteca principal que proporciona las herramientas para los modelos de difusión.\n","# 'transformers' se usa para manejar los codificadores de texto que interpretan nuestros prompts.\n","# 'accelerate' ayuda a optimizar el rendimiento del modelo en diferentes dispositivos (como GPUs).\n","# '-q' al final significa 'quiet', para que la instalación sea menos verbosa.\n","!pip install diffusers transformers accelerate -q\n","\n","# 'torch' es la biblioteca fundamental de PyTorch, necesaria para las operaciones de tensores y redes neuronales.\n","!pip install torch -q\n","\n","# 'scipy' es una biblioteca para computación científica, a veces necesaria para algunas funcionalidades internas.\n","!pip install --upgrade scipy"]},{"cell_type":"code","execution_count":null,"id":"paso-2-importar-bibliotecas","metadata":{"id":"paso-2-importar-bibliotecas"},"outputs":[],"source":["# Paso 2: Importar las bibliotecas necesarias para nuestro script\n","# StableDiffusionPipeline es la clase principal de diffusers para cargar y ejecutar los modelos de Stable Diffusion.\n","from diffusers import StableDiffusionPipeline\n","\n","# torch es la biblioteca de PyTorch, utilizada aquí para especificar el tipo de datos (ej. float16).\n","import torch\n","\n","# IPython.display.display nos permite mostrar imágenes directamente en el cuaderno de Colab.\n","from IPython.display import display\n","\n","# PIL (Pillow) es una biblioteca de procesamiento de imágenes que se utiliza para manipular y mostrar las imágenes generadas.\n","from PIL import Image"]},{"cell_type":"markdown","id":"primer-modelo-v1.5","metadata":{"id":"primer-modelo-v1.5"},"source":["### 2. Primer Modelo: Stable Diffusion V1.5\n","Comenzaremos explorando **Stable Diffusion V1.5**, una versión clásica y ampliamente utilizada. Este modelo es una excelente introducción a la generación de imágenes a partir de texto utilizando técnicas de difusión latente."]},{"cell_type":"code","execution_count":null,"id":"paso-3-cargar-modelo-v1.5","metadata":{"id":"paso-3-cargar-modelo-v1.5"},"outputs":[],"source":["# Paso 3: Cargar el modelo de difusión estable (Stable Diffusion V1.5)\n","# 'runwayml/stable-diffusion-v1-5' es el identificador del modelo en Hugging Face Hub.\n","# from_pretrained() descarga y carga los pesos del modelo.\n","pipe = StableDiffusionPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\")\n","\n","# Movemos el modelo a la GPU para acelerar el procesamiento.\n","# Si no tienes GPU o prefieres usar CPU, puedes cambiar 'cuda' a 'cpu'.\n","pipe.to(\"cuda\")\n"]},{"cell_type":"code","execution_count":null,"id":"paso-4-funcion-generar-v1.5","metadata":{"id":"paso-4-funcion-generar-v1.5"},"outputs":[],"source":["# Paso 4: Definir una función para generar imágenes\n","def generar_imagen_v1_5(prompt):\n","    # El método 'pipe()' es el que ejecuta el proceso de difusión para generar la imagen.\n","    # - prompt: La descripción de texto de la imagen que queremos generar.\n","    # - num_inference_steps: El número de pasos en el proceso de des-difusión.\n","    #   Más pasos generalmente resultan en mejor calidad, pero tardan más tiempo.\n","    # - guidance_scale: Controla qué tan fiel es el resultado al prompt.\n","    #   Valores más altos hacen que la imagen se adhiera más al prompt, pero pueden reducir la diversidad.\n","    images = pipe(prompt, num_inference_steps=30, guidance_scale=7.5).images\n","    return images\n"]},{"cell_type":"code","execution_count":null,"id":"paso-5-prompt-v1.5","metadata":{"id":"paso-5-prompt-v1.5"},"outputs":[],"source":["# Paso 5: Definir nuestro 'prompt' (descripción textual) para la generación de la imagen.\n","prompt_v1_5 = \"A highly detailed portrait of a person, photographic style, in natural lighting, with expressive eyes and soft shadows\""]},{"cell_type":"code","execution_count":null,"id":"paso-6-generar-mostrar-v1.5","metadata":{"id":"paso-6-generar-mostrar-v1.5"},"outputs":[],"source":["# Paso 6: Generar y mostrar la imagen con Stable Diffusion V1.5\n","print(f\"Generando imagen con el prompt: '{prompt_v1_5}' (Stable Diffusion V1.5)...\")\n","images_v1_5 = generar_imagen_v1_5(prompt_v1_5)\n","\n","# Iteramos sobre la lista de imágenes generadas (aunque en este caso es solo una) y las mostramos.\n","for im in images_v1_5:\n","    display(im)\n","print(\"¡Generación completada!\")"]},{"cell_type":"markdown","id":"explicacion-v1.5","metadata":{"id":"explicacion-v1.5"},"source":["### Explicación de Stable Diffusion V1.5\n","Este modelo genera imágenes a partir de texto a través de un proceso de difusión latente. El modelo comienza con una imagen de ruido y, mediante un proceso de des-difusión progresiva, reconstruye la imagen basándose en la descripción proporcionada en el `prompt`.\n","Los parámetros clave que hemos utilizado son:\n","*   `guidance_scale`: Controla cuán fiel es el resultado al prompt. Valores más altos (como 7.5) hacen que la imagen se adhiera más a la descripción textual, pero pueden reducir la diversidad y, en ocasiones, introducir artefactos. Valores más bajos permiten al modelo más libertad creativa.\n","*   `num_inference_steps`: Define el número de pasos en el proceso de inferencia (des-difusión). Más pasos (ej., 50 o 100) generalmente resultan en imágenes de mayor calidad y detalle, pero aumentan el tiempo de procesamiento. Menos pasos (ej., 20-30) son más rápidos pero pueden producir imágenes menos refinadas."]},{"cell_type":"markdown","id":"segundo-modelo-v2.1","metadata":{"id":"segundo-modelo-v2.1"},"source":["### 3. Segundo Modelo: Stable Diffusion 2.1\n","Ahora, vamos a trabajar con una versión más avanzada: **Stable Diffusion 2.1**. Este modelo ofrece una calidad de imagen mejorada y una mayor capacidad para entender prompts detallados. Además, utilizaremos una optimización clave: la **precisión FP16**."]},{"cell_type":"markdown","id":"ventajas-v2.1","metadata":{"id":"ventajas-v2.1"},"source":["#### Ventajas de usar Stable Diffusion 2.1\n","-   **Calidad mejorada:** Produce imágenes más complejas, detalladas y, a menudo, de mayor resolución en comparación con las versiones anteriores (como V1.5).\n","-   **Eficiencia de memoria (con FP16):** Al usar la precisión `FP16` (Floating Point 16), el modelo utiliza menos memoria de la GPU. Esto es crucial en entornos con recursos limitados como Google Colab, permitiendo generar imágenes más grandes o ejecutar modelos más complejos.\n","-   **Manejo de prompts:** Puede interpretar y adherirse mejor a prompts más largos y descriptivos."]},{"cell_type":"code","execution_count":null,"id":"cargar-modelo-v2.1","metadata":{"id":"cargar-modelo-v2.1"},"outputs":[],"source":["# Cargamos el modelo preentrenado de Stable Diffusion 2.1\n","# Especificamos 'torch_dtype=torch.float16' para usar la precisión de punto flotante de 16 bits.\n","# Esto reduce a la mitad el uso de memoria del modelo en la GPU, lo que es ideal para Colab.\n","pipe = StableDiffusionPipeline.from_pretrained(\"stabilityai/stable-diffusion-2-1\", torch_dtype=torch.float16)\n","\n","# Movemos el modelo a la GPU para mejorar el rendimiento.\n","# Si no tienes GPU, cambiar a 'cpu' hará que se ejecute en la CPU (será más lento).\n","pipe.to(\"cuda\")\n"]},{"cell_type":"code","execution_count":null,"id":"funcion-generar-v2.1","metadata":{"id":"funcion-generar-v2.1"},"outputs":[],"source":["# Definimos una función para generar imágenes a partir de un prompt (similar a la anterior, pero específica para este modelo cargado)\n","def generar_imagen_v2_1(prompt):\n","    # Usamos los mismos parámetros de inferencia por defecto para comparar.\n","    images = pipe(prompt, num_inference_steps=30, guidance_scale=7.5).images\n","    return images\n"]},{"cell_type":"code","execution_count":null,"id":"prompt-v2.1","metadata":{"id":"prompt-v2.1"},"outputs":[],"source":["# Prompt de ejemplo para un estilo fotográfico específico con Stable Diffusion 2.1\n","prompt_v2_1 = (\n","    \"A highly detailed portrait of a person, photographic style, in natural lighting, with expressive eyes and soft shadows\"\n",")\n"]},{"cell_type":"code","execution_count":null,"id":"generar-mostrar-v2.1","metadata":{"id":"generar-mostrar-v2.1"},"outputs":[],"source":["# Generamos y mostramos la imagen con Stable Diffusion 2.1\n","print(f\"Generando imagen con el prompt: '{prompt_v2_1}' (Stable Diffusion 2.1)...\")\n","images_v2_1 = generar_imagen_v2_1(prompt_v2_1)\n","\n","for img in images_v2_1:\n","    display(img)\n","print(\"¡Generación completada!\")\n"]},{"cell_type":"markdown","id":"comparacion-v1.5-v2.1","metadata":{"id":"comparacion-v1.5-v2.1"},"source":["### 4. Comparación entre Stable Diffusion V1.5 y Stable Diffusion 2.1\n","La principal diferencia entre ambos modelos es que **Stable Diffusion 2.1** tiene mayor capacidad para capturar detalles y producir imágenes más complejas y de mayor resolución. Stable Diffusion 2.1 también maneja `prompts` más largos y descriptivos mejor que la versión V1.5.\n","\n","Un `prompt` comparativo que podrían probar en ambos modelos para ver estas diferencias visualmente podría ser:\n","\n","`A baroque-style portrait of a person, painted in the style of Caravaggio, with dramatic chiaroscuro, deep shadows, and warm highlights`"]},{"cell_type":"markdown","id":"ajustando-parametros","metadata":{"id":"ajustando-parametros"},"source":["## Ajustando Parámetros de Generación\n","Los modelos de difusión ofrecen varios parámetros que podemos ajustar para controlar el proceso de generación y obtener diferentes resultados. Aquí exploraremos algunos de los más comunes."]},{"cell_type":"markdown","id":"optimizacion-memoria-res","metadata":{"id":"optimizacion-memoria-res"},"source":["### 1. Optimización del Uso de Memoria (Ajustando la Resolución)\n","Para evitar problemas de memoria en Google Colab, especialmente con GPUs limitadas, podemos reducir el tamaño de las imágenes generadas. Esto es particularmente útil cuando necesitamos generar múltiples imágenes o si la GPU se queda sin memoria con la resolución por defecto."]},{"cell_type":"code","execution_count":null,"id":"ajustar-resolucion","metadata":{"id":"ajustar-resolucion"},"outputs":[],"source":["# Ajustamos las dimensiones de las imágenes generadas usando 'height' y 'width'.\n","# Las dimensiones por defecto suelen ser 512x512 o 768x768 dependiendo del modelo.\n","# Reducir esto a 256x256 consumirá mucha menos memoria de la GPU.\n","print(f\"Generando imagen a baja resolución (256x256) con el prompt: '{prompt_v2_1}'...\")\n","images_low_res = pipe(prompt_v2_1, height=512, width=512).images\n","\n","# Mostramos las imágenes generadas\n","for img in images_low_res:\n","    display(img)\n","print(\"¡Generación de baja resolución completada!\")\n"]},{"cell_type":"markdown","id":"ajustes-inferencia-steps","metadata":{"id":"ajustes-inferencia-steps"},"source":["### 2. Ajustando el Número de Pasos de Inferencia (`num_inference_steps`)\n","Ya vimos que `num_inference_steps` define cuántos pasos de des-difusión realiza el modelo. Más pasos generalmente mejoran la calidad de la imagen y el detalle, pero también aumentan el tiempo de procesamiento y el uso de recursos."]},{"cell_type":"code","execution_count":null,"id":"ajustar-pasos-inferencia","metadata":{"id":"ajustar-pasos-inferencia"},"outputs":[],"source":["# Ajustamos el número de pasos de inferencia a 100 para una mayor calidad.\n","# Comparar esta imagen con la generada con 50 pasos para notar la diferencia en detalle.\n","print(f\"Generando imagen con 100 pasos de inferencia con el prompt: '{prompt_v2_1}'...\")\n","images_100_steps = pipe(prompt_v2_1, num_inference_steps=50, guidance_scale=5.0).images\n","\n","# Mostramos las imágenes generadas\n","for img in images_100_steps:\n","    display(img)\n","print(\"¡Generación con 100 pasos completada!\")\n"]},{"cell_type":"markdown","id":"generar-multiples-imagenes","metadata":{"id":"generar-multiples-imagenes"},"source":["### 3. Generar Múltiples Imágenes Simultáneamente (`num_images_per_prompt`)\n","Podemos generar varias imágenes al mismo tiempo a partir de un mismo `prompt` ajustando el parámetro `num_images_per_prompt`. Esto es útil para explorar diferentes interpretaciones del modelo y seleccionar la mejor."]},{"cell_type":"code","execution_count":null,"id":"multiples-imagenes","metadata":{"id":"multiples-imagenes"},"outputs":[],"source":["# Generar 3 imágenes a la vez con el mismo prompt.\n","# Cada imagen será una variación diferente, ya que el punto de partida es ruido aleatorio.\n","print(f\"Generando 3 imágenes simultáneamente con el prompt: '{prompt_v2_1}'...\")\n","images_multiples = pipe(prompt_v2_1, num_images_per_prompt=3).images\n","\n","# Mostramos las imágenes generadas\n","for img in images_multiples:\n","    display(img)\n","print(\"¡Generación de múltiples imágenes completada!\")\n"]},{"cell_type":"markdown","id":"seccion-extra-sdxl","metadata":{"id":"seccion-extra-sdxl"},"source":["## Extra: Explorando Stable Diffusion XL (SDXL)\n","Stable Diffusion XL (SDXL) representa la última generación de modelos de Stable Diffusion y ofrece una calidad de imagen aún superior, mayor resolución nativa y una mejor comprensión de `prompts` complejos. Es ideal para aplicaciones profesionales y resultados de alta fidelidad."]},{"cell_type":"code","execution_count":null,"id":"sdxl-imports","metadata":{"id":"sdxl-imports"},"outputs":[],"source":["# Aunque ya importamos algunas librerías, aquí re-importamos las específicas para SDXL para mayor claridad del bloque.\n","import torch\n","from IPython.display import display\n","from PIL import Image\n"]},{"cell_type":"code","execution_count":null,"id":"sdxl-pipeline-import","metadata":{"id":"sdxl-pipeline-import"},"outputs":[],"source":["# Importamos la clase específica para usar Stable Diffusion XL.\n","# SDXL tiene una estructura de pipeline ligeramente diferente para manejar sus capacidades avanzadas.\n","from diffusers import StableDiffusionXLPipeline\n"]},{"cell_type":"code","execution_count":null,"id":"sdxl-cargar-modelo","metadata":{"colab":{"background_save":true},"id":"sdxl-cargar-modelo"},"outputs":[],"source":["# Cargamos el modelo preentrenado de Stable Diffusion XL (SDXL Base 1.0).\n","# Este modelo es significativamente más grande y potente que las versiones anteriores.\n","# Nuevamente, usamos `torch_dtype=torch.float16` para optimizar el uso de memoria en la GPU.\n","print(\"Cargando Stable Diffusion XL (SDXL Base 1.0). Esto puede tardar unos minutos...\")\n","pipe_xl = StableDiffusionXLPipeline.from_pretrained(\n","    \"stabilityai/stable-diffusion-xl-base-1.0\",\n","    torch_dtype=torch.float16\n",")\n"]},{"cell_type":"code","execution_count":null,"id":"sdxl-to-cuda","metadata":{"id":"sdxl-to-cuda"},"outputs":[],"source":["# Movemos el modelo a la GPU para aprovechar los recursos y hacer el procesamiento más rápido.\n","pipe_xl.to(\"cuda\")\n","print(\"Modelo SDXL cargado y listo en la GPU.\")"]},{"cell_type":"code","execution_count":null,"id":"sdxl-funcion-generar","metadata":{"id":"sdxl-funcion-generar"},"outputs":[],"source":["# Definimos una función para generar imágenes específicamente con Stable Diffusion XL.\n","def generar_imagen_xl(prompt, num_images=1):\n","    # SDXL a menudo rinde mejor con un guidance_scale ligeramente más bajo o similar a 2.1.\n","    # Los num_inference_steps son también importantes para la calidad.\n","    images = pipe_xl(prompt, num_inference_steps=50, guidance_scale=7.5).images\n","    return images"]},{"cell_type":"code","execution_count":null,"id":"sdxl-prompt","metadata":{"id":"sdxl-prompt"},"outputs":[],"source":["# Aquí puedes pedir a los estudiantes que escriban su propio prompt o usar uno predefinido.\n","# Este prompt es el mismo que usamos para V2.1 para facilitar la comparación de calidad.\n","prompt_xl = (\n","    \"A calotype in black and white showing a swimming pool with people swimming, captured in dramatic lighting by Caravaggio, \"\n","    \"then colored with the vibrant and bright style of David Hockney. The scene is nostalgic yet energetic, blending classic and modern elements.\"\n",")\n"]},{"cell_type":"code","execution_count":null,"id":"sdxl-generar-mostrar","metadata":{"id":"sdxl-generar-mostrar"},"outputs":[],"source":["# Llamamos a la función para generar la imagen con Stable Diffusion XL\n","print(f\"Generando imagen con SDXL para el prompt: '{prompt_xl}'...\")\n","images_xl = generar_imagen_xl(prompt_xl)\n","\n","# Mostramos la imagen generada\n","for img in images_xl:\n","    display(img)\n","print(\"¡Generación con SDXL completada! Observa la mejora en detalle y realismo.\")\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":5}